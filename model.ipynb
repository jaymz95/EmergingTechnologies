{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__future__`\n",
    "is a real module, and serves three purposes:\n",
    "- To avoid confusing existing tools that analyze import statements and expect to find the modules they’re importing. (ref 'https://docs.python.org/3/library/__future__.html')\n",
    "\n",
    "### `Keras`\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow. It was developed with a focus on enabling fast experimentation. (ref 'https://keras.io')\n",
    "\n",
    "#### Sequential\n",
    "-The sequential API allows you to create models layer-by-layer for most problems. It is limited in that it does not allow you to create models that share layers or have multiple inputs or outputs. (ref 'https://machinelearningmastery.com/keras-functional-api-deep-learning/')\n",
    "\n",
    "#### Dense\n",
    "-Dense is a name for a Fully connected / linear layer in keras. (ref 'https://forums.fast.ai/t/dense-vs-convolutional-vs-fully-connected-layers/191')\n",
    "\n",
    "#### Dropout\n",
    "- Dropout is a technique where randomly selected neurons are ignored during training. (ref 'https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/')\n",
    "\n",
    "#### Flatten\n",
    "- The purpose of this argument is to preserve weight ordering when switching a model from one data format to another. (ref 'https://keras.io/layers/core/')\n",
    "\n",
    "#### Conv2D\n",
    "- This layer creates a combination kernel that is combined with the layer input to produce a tensor of outputs. If 'use_bias' is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. (ref 'https://keras.io/layers/convolutional/')\n",
    "\n",
    "#### MaxPooling2D\n",
    "- Max pooling operation for spatial data. (ref 'https://keras.io/layers/pooling/#maxpooling2d') \n",
    "\n",
    "#### Backend (Tensorflow)\n",
    "- keras relies on a specialized, well optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. (ref 'https://keras.io/backend/') \n",
    "\n",
    "\n",
    "### `MNIST`\n",
    "The MNIST database of handwritten digits, available from keras, has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image. (ref 'http://yann.lecun.com/exdb/mnist/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refference \"https://www.pytorials.com/deploy-keras-model-to-production-using-flask/\"\n",
    "\n",
    "from __future__ import print_function\n",
    "#simplified interface for building models \n",
    "import keras\n",
    "#our handwritten character labeled dataset (28x28 images of numbers 0-9)\n",
    "from keras.datasets import mnist\n",
    "#because our models are simple\n",
    "from keras.models import Sequential\n",
    "#dense means fully connected layers, dropout is a technique to improve convergence, flatten to reshape our matrices for feeding\n",
    "#into respective layers\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "#for convolution (images) and pooling is a technique to help choose the most relevant features in an image\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent \n",
    "In machine learning, we use gradient descent to update the parameters of our model. (ref 'https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html')\n",
    "\n",
    "### Epochs\n",
    "One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.\n",
    "Since one epoch is too big to feed to the computer at once we divide it in several smaller batches. (ref 'https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9')\n",
    "\n",
    "### Batch Size\n",
    "Total number of training examples present in a single batch.\n",
    "Note: Batch size and number of batches are two different things.\n",
    "But What is a Batch?\n",
    "As I said, you can’t pass the entire dataset into the neural net at once. So, you divide dataset into Number of Batches or sets or parts. (ref 'https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mini batch gradient descent\n",
    "batch_size = 128\n",
    "#10 difference characters 0-9\n",
    "num_classes = 10\n",
    "#very short training time\n",
    "\n",
    "# Epoch: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", \n",
    "# used to separate training into distinct phases, which is useful for logging and periodic evaluation. \n",
    "# When using evaluation_data or evaluation_split with the fit method of Keras models, \n",
    "# evaluation will be run at the end of every epoch\n",
    "# (ref http://faroit.com/keras-docs/2.0.2/getting-started/faq/)\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "# 28x28 pixel images. \n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refference \"https://www.pytorials.com/deploy-keras-model-to-production-using-flask/\"\n",
    "\n",
    "# the data downloaded, shuffled and split between train and test sets (imported and formated)\n",
    "# image data unloaded from mnist into the variables on the left\n",
    "\n",
    "# The MNIST database contains 60,000 training images and 10,000 testing images taken from \n",
    "# American Census Bureau employees and American high school students\n",
    "# (ref https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d)\n",
    "\n",
    "# Therefore, in the second line, I have separated these two groups as train and test and \n",
    "# also separated the labels and the images. x_train and x_test parts contain greyscale RGB codes (from 0 to 255) \n",
    "# while y_train and y_test parts contains labels from 0 to 9 which represents which number they actually are\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# made trainig slower so commented out\n",
    "# Normalization is a rescaling of the data from the original range so that all values are within the range of 0 and 1.\n",
    "#x_train = keras.utils.normalize(x_train, axis = 1)\n",
    "#x_test = keras.utils.normalize(x_test, axis = 1)\n",
    "\n",
    "#this assumes our data format\n",
    "#For 3D data, \"channels_last\" assumes (conv_dim1, conv_dim2, conv_dim3, channels) while \n",
    "#\"channels_first\" assumes (channels, conv_dim1, conv_dim2, conv_dim3).\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    \n",
    "    # a full-color image with all 3 RGB channels will have a depth of 3.\n",
    "    # Our MNIST images only have a depth of 1, but we must explicitly declare that.\n",
    "    # In other words, we want to transform our dataset from having shape (n, width, height) to (n, depth, width, height)\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "#more reshaping\n",
    "x_train = x_train.astype('float32') # Converts to float\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255 # rescaling of the data from the original range so that all values are within the range of 0 and 1.\n",
    "x_test /= 255 # (Normalizing)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sequential model is a linear stack of layers.\n",
    "# You can create a Sequential model by passing a list of layer instances to the constructor\n",
    "model = Sequential()\n",
    "# You can also simply add layers via the .add() method using Sequential\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#choose the best features via pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#randomly turn neurons on and off to improve convergence\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#flatten since too many dimensions, we only want a classification output\n",
    "# Flattening a tensor means to remove all of the dimensions except for one\n",
    "# condensing to a one dimentional array\n",
    "model.add(Flatten())\n",
    "\n",
    "#fully connected to get all relevant data\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#one more dropout for convergence' sake :) \n",
    "model.add(Dropout(0.5))\n",
    "#output a softmax to squash the matrix into output probabilities\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "#Adaptive learning rate (adaDelta) is a popular form of gradient descent rivaled only by adam and adagrad\n",
    "#categorical ce since we have multiple classes (10) \n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 85s 1ms/step - loss: 0.2551 - acc: 0.9217 - val_loss: 0.0513 - val_acc: 0.9836\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.0863 - acc: 0.9744 - val_loss: 0.0395 - val_acc: 0.9867\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.0655 - acc: 0.9804 - val_loss: 0.0351 - val_acc: 0.9883\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0534 - acc: 0.9841 - val_loss: 0.0293 - val_acc: 0.9906\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 87s 1ms/step - loss: 0.0453 - acc: 0.9863 - val_loss: 0.0292 - val_acc: 0.9897\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.0402 - acc: 0.9884 - val_loss: 0.0282 - val_acc: 0.9904\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 85s 1ms/step - loss: 0.0358 - acc: 0.9890 - val_loss: 0.0255 - val_acc: 0.9915\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.0328 - acc: 0.9899 - val_loss: 0.0251 - val_acc: 0.9916\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 82s 1ms/step - loss: 0.0292 - acc: 0.9911 - val_loss: 0.0264 - val_acc: 0.9918\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.0276 - acc: 0.9914 - val_loss: 0.0272 - val_acc: 0.9921\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.0257 - acc: 0.9921 - val_loss: 0.0285 - val_acc: 0.9909\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.0237 - acc: 0.9927 - val_loss: 0.0236 - val_acc: 0.9926\n",
      "Test loss: 0.023570960051386647\n",
      "Test accuracy: 0.9926\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1, # verbose = 1, will show you an animated progress bar\n",
    "          validation_data=(x_test, y_test))\n",
    " # prints outs for loss and accuracy\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#Save the model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
